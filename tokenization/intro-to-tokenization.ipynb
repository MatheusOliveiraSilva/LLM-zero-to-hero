{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67db846ac8b2bd18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization is the process of dividing a text into smaller linguistic units known as \"tokens\". These tokens are often words, but can include punctuation, numbers, and other symbols depending on the application. Tokenization is a fundamental first step in text preprocessing for NLP.\n",
    "\n",
    "There are different tokenization approaches, from simple space separation to more complex methods that consider linguistic rules or use statistical models. Examples:\n",
    "\n",
    "1. Tokenization by spaces: Split the text whenever it finds a space (\" \").\n",
    "2. Tokenization by punctuation: Also separate punctuation marks as independent tokens.\n",
    "3. Tokenization by linguistic rules (n-grams, subword tokenization, etc.): Use more advanced techniques to deal with languages ​​with many compound words or that do not use spacing, or to deal with vocabulary reduction in deep learning applications (e.g. byte-pair encoding - BPE)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57792df1285dbf4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stopwords Removal and Text Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b9bc86bf525364"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stopwords are very common words in a language, which generally do not add much meaning to the text, such as \"o\", \"a\", \"de\", \"in\" (in the case of Portuguese), or \"the\", \"a\", \"and\", \"of\" (in English). Removing stopwords can help reduce the dimensionality of the text and focus on the most relevant words.\n",
    "\n",
    "Text normalization includes techniques such as:\n",
    "- Lemmatization: Reducing words to their canonical form (lemma). Ex: \"run\", \"running\", \"ran\" → \"run\".\n",
    "- Stemming: Reducing words to their stem, usually cutting suffixes. Ex: \"run\", \"running\", \"rush\" → \"corr\".\n",
    "- Lowercase conversion: Convert all words to lowercase.\n",
    "- Removing punctuation and special symbols: Cleaning the text, removing characters that do not carry semantic meaning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dcd268fd1df877b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# N-grams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20308005a6078a7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "N-grams are continuous sequences of N tokens within a text. For example:\n",
    "- Unigrams (N=1): [\"This\", \"is\", \"one\", \"example\"]\n",
    "- Bigrams (N=2): [\"This is\", \"is an\", \"an example\"\n",
    "- Trigrams (N=3): [\"This is one\", \"is an example\"]\n",
    "\n",
    "They are useful for capturing context: while unigrams only consider individual words, bigrams and trigrams can capture immediate relationships between adjacent words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6f75a4f2525bef1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pratice\n",
    "\n",
    "Below is an example of preprocessing in Python using libraries such as re (regular expressions) for cleaning, and nltk for tokenization and stopword removal. Text normalization, n-gram generation and other steps will also be illustrated."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f3332c40b0751dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T17:59:39.772344Z",
     "start_time": "2024-12-17T17:59:39.766246Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"This is a simple example of text. The main objective is to clean some tokens and form n-grams.\"\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "text = re.sub(r'[^\\w\\s]', '', text)  # remove everything that isnt word or blank space\n",
    "\n",
    "tokens = word_tokenize(text, language='english')\n",
    "\n",
    "stopwords_portugues = set(stopwords.words('english'))\n",
    "tokens_sem_stop = [t for t in tokens if t not in stopwords_portugues]\n",
    "\n",
    "bigrams = list(ngrams(tokens_sem_stop, 2))\n",
    "trigrams = list(ngrams(tokens_sem_stop, 3))\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Tokens without stopwords:\", tokens_sem_stop)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
